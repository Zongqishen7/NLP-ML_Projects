{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 6: Clustering and Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you'll practice different text clustering methods. A dataset has been prepared for you:\n",
    "- `hw6_train.csv`: This file contains a list of documents. It's used for training models\n",
    "- `hw6_test`: This file contains a list of documents and their ground-truth labels (4 lables: 1,2,3,7). It's used for external evaluation. \n",
    "\n",
    "|Text| Label|\n",
    "|----|-------|\n",
    "|paraglider collides with hot air balloon ... | 1|\n",
    "|faa issues fire warning for lithium ... | 2|\n",
    "| .... |...|\n",
    "\n",
    "Sample outputs have been provided to you. Due to randomness, you may not get the same result as shown here. Your taget is to achieve about 70% F1 for the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: K-Mean Clustering \n",
    "\n",
    "Define a function `cluster_kmean(train_text, test_text, text_label)` as follows:\n",
    "- Take three inputs: \n",
    "    - `train_text` is a list of documents for traing \n",
    "    - `test_text` is a list of documents for test\n",
    "    - `test_label` is the labels corresponding to documents in `test_text` \n",
    "- First generate `TFIDF` weights. You need to decide appropriate values for parameters such as `stopwords` and `min_df`:\n",
    "    - Keep or remove stopwords? Customized stop words? \n",
    "    - Set appropriate `min_df` to filter infrequent words\n",
    "- Use `KMeans` to cluster documents in `train_text` into 4 clusters. Here you need to decide the following parameters:\n",
    "    \n",
    "    - Distance measure: `cosine similarity`  or `Euclidean distance`? Pick the one which gives you better performance.  \n",
    "    - When clustering, be sure to  use sufficient iterations with different initial centroids to make sure clustering converge.\n",
    "- Test the clustering model performance using `test_label` as follows: \n",
    "  - Predict the cluster ID for each document in `test_text`.\n",
    "  - Apply `majority vote` rule to dynamically map the predicted cluster IDs to `test_label`. Note, you'd better not hardcode the mapping, because cluster IDs may be assigned differently in each run. (hint: if you use pandas, look for `idxmax` function).\n",
    "  - print out the classification report for the test subset \n",
    "  \n",
    "  \n",
    "- This function has no return. Print out the classification report. \n",
    "\n",
    "\n",
    "- Briefly discuss:\n",
    "    - How did you choose tfidf parameters?\n",
    "    - Which distance measure is better and why is it better?\n",
    "    - Could you assign a meaningful name to each cluster? Discuss how you interpret each cluster.\n",
    "- You can write your analysis in the same notebook or in a separate document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your import statement\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn import mixture\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Would you rather get a gift that you knew what...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is the internet ruining people's ability to co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Permanganate?\\nSuppose permanganate was used t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If Rock-n-Roll is really the work of the devil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Has anyone purchased software to watch TV on y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Would you rather get a gift that you knew what...\n",
       "1  Is the internet ruining people's ability to co...\n",
       "2  Permanganate?\\nSuppose permanganate was used t...\n",
       "3  If Rock-n-Roll is really the work of the devil...\n",
       "4  Has anyone purchased software to watch TV on y..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"hw6_train.csv\")\n",
    "train_text=train[\"text\"]\n",
    "\n",
    "test = pd.read_csv(\"hw6_test.csv\")\n",
    "test_label = test[\"label\"]\n",
    "test_text = test[\"text\"]\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_kmean(train_text, test_text, test_label):\n",
    "    \n",
    "    tfidf_vect = TfidfVectorizer(stop_words=\"english\",\\\n",
    "                             min_df=2) \n",
    "    dtm= tfidf_vect.fit_transform(train_text)\n",
    "    km = KMeans(n_clusters=4, n_init=20, random_state = 2).fit(dtm)\n",
    "    clusters = km.labels_.tolist()\n",
    "    test_dtm = tfidf_vect.transform(test_text)\n",
    "    predicted = km.predict(test_dtm)\n",
    "    confusion_df = pd.DataFrame(list(zip(test_label.values, predicted)),\\\n",
    "                            columns = [\"label\", \"cluster\"])\n",
    "    cluster_dict={0:1,1:7,2:3,3:2}\n",
    "    predicted_target=[cluster_dict[i] \\\n",
    "                      for i in predicted]\n",
    "    print(metrics.classification_report\\\n",
    "      (test[\"label\"], predicted_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.14      0.32      0.20       332\n",
      "           2       0.06      0.07      0.06       314\n",
      "           3       0.98      0.15      0.27       355\n",
      "           7       0.01      0.00      0.01       273\n",
      "\n",
      "    accuracy                           0.14      1274\n",
      "   macro avg       0.30      0.14      0.13      1274\n",
      "weighted avg       0.33      0.14      0.14      1274\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_kmean(train_text, test_text, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Clustering by Gaussian Mixture Model\n",
    "\n",
    "In this task, you'll re-do the clustering using a Gaussian Mixture Model. Call this function  `cluster_gmm(train_text, test_text, text_label)`. \n",
    "\n",
    "You may take a subset of data to do GMM because it can take a lot of time. \n",
    "\n",
    "Write your analysis on the following:\n",
    "- How did you pick the parameters such as the number of clusters, variance type etc.?\n",
    "- Compare to Kmeans in Q1, do you achieve better preformance by GMM? \n",
    "\n",
    "- Note, like KMean, be sure to use different initial means (i.e. `n_init` parameter) when fitting the model to achieve the model stability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_gmm(train_text, test_text, test_label):\n",
    "    \n",
    "    tfidf_vect = TfidfVectorizer(stop_words=\"english\",\\\n",
    "                             min_df=2) \n",
    "    test_dtm = tfidf_vect.fit_transform(test_text)\n",
    "    best_gmm = mixture.GaussianMixture(covariance_type='diag', init_params='kmeans', max_iter=100,\n",
    "        means_init=None, n_components=4, n_init=1, precisions_init=None,\n",
    "        random_state=42, reg_covar=1e-06, tol=0.001, verbose=0,\n",
    "        verbose_interval=10, warm_start=False, weights_init=None).fit(test_dtm.toarray())\n",
    "    predicted = best_gmm.predict(test_dtm.toarray())\n",
    "    cluster_dict={0:2, 1:7,2:1,3:3}\n",
    "\n",
    "    predicted_target=[cluster_dict[i] \\\n",
    "                      for i in predicted]\n",
    "\n",
    "    print(metrics.classification_report\\\n",
    "          (test_label, predicted_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.33      0.48       332\n",
      "           2       0.40      0.24      0.30       314\n",
      "           3       0.29      0.51      0.37       355\n",
      "           7       0.23      0.29      0.26       273\n",
      "\n",
      "    accuracy                           0.35      1274\n",
      "   macro avg       0.46      0.34      0.35      1274\n",
      "weighted avg       0.47      0.35      0.36      1274\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_gmm(train_text, test_text, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Clustering by LDA \n",
    "\n",
    "In this task, you'll re-do the clustering using LDA. Call this function `cluster_lda(train_text, test_text, text_label)`. \n",
    "\n",
    "However, since LDA returns topic mixture for each document, you `assign the topic with highest probability to each test document`, and then measure the performance as in Q1\n",
    "\n",
    "In addition, within the function, please print out the top 30 words for each topic\n",
    "\n",
    "Finally, please analyze the following:\n",
    "- Based on the top words of each topic, could you assign a meaningful name to each topic?\n",
    "- Although the test subset shows there are 4 clusters, without this information, how do you choose the number of topics? \n",
    "- Does your LDA model achieve better performance than KMeans or GMM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_lda(train_text, test_text, test_label):\n",
    "\n",
    "    text=list(test_text)\n",
    "    label=list(test_label)\n",
    "\n",
    "    tf_vectorizer = CountVectorizer(stop_words=\"english\",\\\n",
    "                                 min_df=2)\n",
    "    tf = tf_vectorizer.fit_transform(text)\n",
    "\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    X_train, X_test = train_test_split(\\\n",
    "                    tf, test_size=0.1, random_state=0)\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=4, \\\n",
    "                                    max_iter=30,verbose=1,\n",
    "                                    evaluate_every=1, n_jobs=1,\n",
    "                                    random_state=0).fit(X_train)\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        # print out top 20 words per topic \n",
    "        words=[(tf_feature_names[i],'%.2f'%topic[i]) \\\n",
    "               for i in topic.argsort()[::-1][0:20]]\n",
    "        print(words)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    tfidf_vect = TfidfVectorizer(stop_words=\"english\",\\\n",
    "                             min_df=2) \n",
    "    dtm= tfidf_vect.fit_transform(train_text)\n",
    "    km = KMeans(n_clusters=4, n_init=20, random_state = 2).fit(dtm)\n",
    "    clusters = km.labels_.tolist()\n",
    "    test_dtm = tfidf_vect.transform(test_text)\n",
    "    predicted = km.predict(test_dtm)\n",
    "    confusion_df = pd.DataFrame(list(zip(test_label.values, predicted)),\\\n",
    "                            columns = [\"label\", \"cluster\"])\n",
    "    cluster_dict={0:1,1:7,2:3,3:2}\n",
    "    predicted_target=[cluster_dict[i] \\\n",
    "                      for i in predicted]\n",
    "    print(metrics.classification_report\\\n",
    "      (test[\"label\"], predicted_target))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 30, perplexity: 4064.4207\n",
      "iteration: 2 of max_iter: 30, perplexity: 3755.4315\n",
      "iteration: 3 of max_iter: 30, perplexity: 3639.3691\n",
      "iteration: 4 of max_iter: 30, perplexity: 3561.0986\n",
      "iteration: 5 of max_iter: 30, perplexity: 3505.5160\n",
      "iteration: 6 of max_iter: 30, perplexity: 3464.3901\n",
      "iteration: 7 of max_iter: 30, perplexity: 3429.2793\n",
      "iteration: 8 of max_iter: 30, perplexity: 3399.3592\n",
      "iteration: 9 of max_iter: 30, perplexity: 3374.0454\n",
      "iteration: 10 of max_iter: 30, perplexity: 3350.9541\n",
      "iteration: 11 of max_iter: 30, perplexity: 3331.6991\n",
      "iteration: 12 of max_iter: 30, perplexity: 3315.3578\n",
      "iteration: 13 of max_iter: 30, perplexity: 3299.3841\n",
      "iteration: 14 of max_iter: 30, perplexity: 3284.9624\n",
      "iteration: 15 of max_iter: 30, perplexity: 3271.9795\n",
      "iteration: 16 of max_iter: 30, perplexity: 3260.0923\n",
      "iteration: 17 of max_iter: 30, perplexity: 3248.6678\n",
      "iteration: 18 of max_iter: 30, perplexity: 3236.9148\n",
      "iteration: 19 of max_iter: 30, perplexity: 3225.1273\n",
      "iteration: 20 of max_iter: 30, perplexity: 3214.1938\n",
      "iteration: 21 of max_iter: 30, perplexity: 3203.8288\n",
      "iteration: 22 of max_iter: 30, perplexity: 3193.7671\n",
      "iteration: 23 of max_iter: 30, perplexity: 3184.0699\n",
      "iteration: 24 of max_iter: 30, perplexity: 3174.9664\n",
      "iteration: 25 of max_iter: 30, perplexity: 3166.8787\n",
      "iteration: 26 of max_iter: 30, perplexity: 3159.2807\n",
      "iteration: 27 of max_iter: 30, perplexity: 3151.2370\n",
      "iteration: 28 of max_iter: 30, perplexity: 3143.5188\n",
      "iteration: 29 of max_iter: 30, perplexity: 3136.5238\n",
      "iteration: 30 of max_iter: 30, perplexity: 3128.6833\n",
      "Topic 0:\n",
      "[('need', '131.64'), ('money', '125.68'), ('good', '102.40'), ('want', '101.21'), ('business', '98.51'), ('company', '92.87'), ('credit', '92.24'), ('work', '91.00'), ('know', '85.82'), ('people', '80.66'), ('job', '79.05'), ('make', '77.80'), ('just', '72.89'), ('don', '71.93'), ('pay', '69.95'), ('area', '63.39'), ('home', '62.47'), ('help', '59.03'), ('use', '57.82'), ('like', '57.63')]\n",
      "\n",
      "\n",
      "Topic 1:\n",
      "[('like', '177.56'), ('just', '155.58'), ('help', '148.67'), ('weight', '147.16'), ('day', '139.05'), ('good', '124.33'), ('eat', '124.24'), ('don', '122.46'), ('need', '119.10'), ('body', '117.30'), ('time', '116.20'), ('skin', '101.51'), ('doctor', '90.00'), ('use', '88.28'), ('000', '87.65'), ('know', '85.26'), ('really', '83.32'), ('try', '77.14'), ('feel', '74.60'), ('want', '73.42')]\n",
      "\n",
      "\n",
      "Topic 2:\n",
      "[('people', '346.48'), ('god', '328.24'), ('like', '275.12'), ('know', '253.96'), ('think', '253.92'), ('just', '247.54'), ('don', '198.83'), ('time', '165.15'), ('way', '156.83'), ('life', '153.30'), ('believe', '142.42'), ('make', '127.06'), ('really', '126.80'), ('say', '124.32'), ('person', '124.24'), ('world', '123.31'), ('good', '118.87'), ('want', '111.09'), ('does', '109.53'), ('did', '109.01')]\n",
      "\n",
      "\n",
      "Topic 3:\n",
      "[('www', '141.73'), ('nhttp', '104.29'), ('com', '90.94'), ('http', '74.49'), ('does', '70.22'), ('light', '68.12'), ('used', '65.27'), ('water', '65.17'), ('earth', '61.37'), ('energy', '56.83'), ('html', '48.23'), ('speed', '47.80'), ('body', '46.08'), ('air', '45.21'), ('mass', '45.19'), ('bacteria', '44.23'), ('na', '44.08'), ('know', '42.96'), ('org', '42.06'), ('form', '41.30')]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.14      0.32      0.20       332\n",
      "           2       0.06      0.07      0.06       314\n",
      "           3       0.98      0.15      0.27       355\n",
      "           7       0.01      0.00      0.01       273\n",
      "\n",
      "    accuracy                           0.14      1274\n",
      "   macro avg       0.30      0.14      0.13      1274\n",
      "weighted avg       0.33      0.14      0.14      1274\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_lda(train_text, test_text, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 (Bonus): Topic Coherence and Separation\n",
    "\n",
    "For the LDA model you obtained at Q3, can you measure the coherence and separation of topics? Try different model parameters (e.g. number of topics, $\\alpha$) to see which one gives you the best separation and coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
