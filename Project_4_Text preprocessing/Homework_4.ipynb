{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>4: Text preprocessing</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, json, string\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Regular Expression\n",
    "\n",
    "Define a **extract** function which:\n",
    "- Takes a piece of text (in the format of shown below) as an input\n",
    "- Extracts data into a list of tuples using regular expression, e.g.  `[('BTC-USD','56,212.15','-58.16','-0.10%'), ('ETH-USD',  ...), ...]`\n",
    "- Returns the list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Symbol   Last Price  Change   % Change   Note\\n                  BTC-USD  56,212.15   -58.16   -0.10%   Bitcoin \\n                  ETH-USD  1,787.79    -53.63   -2.91%   Ether\\n                  BNB-USD  1,101,290.51      +5.81    +2.04%   Binance\\n                  USDT-USD 1.0003      -0.0004  -0.04%   Tether\\n                  ADA-USD  1.1187      -0.0528  -4.51%   Cardano\\n      '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='''Symbol   Last Price  Change   % Change   Note\n",
    "                  BTC-USD  56,212.15   -58.16   -0.10%   Bitcoin \n",
    "                  ETH-USD  1,787.79    -53.63   -2.91%   Ether\n",
    "                  BNB-USD  1,101,290.51      +5.81    +2.04%   Binance\n",
    "                  USDT-USD 1.0003      -0.0004  -0.04%   Tether\n",
    "                  ADA-USD  1.1187      -0.0528  -4.51%   Cardano\n",
    "      '''\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "\n",
    "def extract(text):\n",
    "    #a=re.findall(r'(\\w+-\\w+)\\s+(\\d+,\\d+.\\d+|\\d+.\\d+)\\s+(\\S\\d+.\\d+)\\s+(\\S\\d+.\\d+%)',text)\n",
    "    tokens = re.findall(r'\\w+-\\w+ |\\W\\d[\\.\\d{1,4}]*[%|\\w]',text)\n",
    "    tokens = [i.replace(\" \",'') for i in tokens]\n",
    "    from nltk import ngrams\n",
    "    n = 4\n",
    "    fourgrams = ngrams(text.split(), n)\n",
    "    fourgrams = list(fourgrams)\n",
    "\n",
    "    list1=[]\n",
    "    for i in range(len(fourgrams)):\n",
    "        if i in [7,12,17,22,27]:\n",
    "            list1.append(fourgrams[i])\n",
    "    a = list1\n",
    "    \n",
    "    return a\n",
    "    \n",
    "    \n",
    "    # add your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BTC-USD', '56,212.15', '-58.16', '-0.10%'),\n",
       " ('ETH-USD', '1,787.79', '-53.63', '-2.91%'),\n",
       " ('BNB-USD', '1,101,290.51', '+5.81', '+2.04%'),\n",
       " ('USDT-USD', '1.0003', '-0.0004', '-0.04%'),\n",
       " ('ADA-USD', '1.1187', '-0.0528', '-4.51%')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "\n",
    "extract(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Collocation\n",
    "\n",
    "Define a function `top_collocation(doc, K)` to find top-K collocations in specific patterns in a document as follows:\n",
    "  - Takes a document (i.e. `doc`) and `K` as inputs\n",
    "  - Find collocations as follows:\n",
    "    - Tokenize the document and find POS tag of each token (hint: you can use NLTK word tokenizer or Spacy tokenizer).\n",
    "    - Create bigrams from the tokens with POS tags and remove a bigram if one of the bigram is punctuation.\n",
    "\n",
    "    - Keep only bigrams matching the following patterns:\n",
    "       - `Adj + Noun`: e.g. linear function\n",
    "       - `Noun + Noun`: e.g. regression coefficient\n",
    "    - Get frequency of each bigram (hint: you can use nltk.FreqDist)\n",
    "    - Returns top K collocations by frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "\n",
    "\n",
    "def top_collocation(doc, K):\n",
    "  \n",
    "    tokens=nltk.word_tokenize(article)\n",
    "\n",
    "# tag each tokenized word\n",
    "    tagged_tokens= nltk.pos_tag(tokens)\n",
    "    bigrams=list(nltk.bigrams(tagged_tokens))\n",
    "    string.punctuation += \"’\"\n",
    "    string.punctuation += \"“\"\n",
    "    bigrams=[i for i in bigrams if not i[0][0] in string.punctuation]\n",
    "    bigrams=[i for i in bigrams if not i[1][0] in string.punctuation]\n",
    "    phrases=[ (x[0],y[0]) for (x,y) in bigrams if (x[1].startswith('JJ') and y[1].startswith('NN'))|(x[1].startswith('NN') and y[1].startswith('NN'))]\n",
    "    result=nltk.FreqDist(phrases)\n",
    "    result= result.most_common(6)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('public', 'health'), 14),\n",
       " (('community', 'spread'), 10),\n",
       " (('United', 'States'), 8),\n",
       " (('higher', 'risk'), 4),\n",
       " (('COVID-19', 'illness'), 4),\n",
       " (('elevated', 'risk'), 4)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json.load(open(\"qa.json\",\"r\"))\n",
    "article = data[\"context\"]\n",
    "\n",
    "top_collocation(article, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Question and Answering (QA) System\n",
    "\n",
    "Develop a QA system. \n",
    "\n",
    "For example, the file `qa.json` contains a research article. This article can answer a number of questions about COVID-19. Design a solution to automatically search answers to these questions in this article.\n",
    "\n",
    "`qa.json` is taken from https://github.com/deepset-ai/COVID-QA. This file contains a few questions, and answers to these questions have been located in the article. I'll define a QA system and check if system can locate the right answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDC Summary 21 MAR 2020,\n",
      "https://www.cdc.gov/coronavirus/2019-ncov/cases-updates/summary.html\n",
      "\n",
      "This is a rapidly evolving situation and CDC will provide updated information and guidance as it becomes \n"
     ]
    }
   ],
   "source": [
    "# Retrieve the article\n",
    "\n",
    "data = json.load(open(\"qa.json\",\"r\"))\n",
    "article = data[\"context\"]\n",
    "\n",
    "# A long article. Just print the first 200 characters\n",
    "print(article[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What age group has the highest rate of severe outcomes?', 'id': 236, 'answers': [{'text': 'people 85 years and older', 'answer_start': 6117}], 'is_impossible': False}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['What age group has the highest rate of severe outcomes?',\n",
       " 'How is COVID-19 spread?',\n",
       " 'How many states in the U.S. have reported cases of COVID-19?',\n",
       " 'When did the White House launch the \"15 Days to Slow the Spread\" program?',\n",
       " 'What should mildly-ill patients do?',\n",
       " 'What type of virus is SARS-CoV-2?',\n",
       " 'What viruses are similar to the COVID-19 coronavirus?',\n",
       " 'What are the phases of a pandemic?',\n",
       " 'At which phase does the peak of the pandemic occur?',\n",
       " 'People with which medical conditions have a higher rate of severe illness?',\n",
       " 'What kind of test can diagnose COVID-19?',\n",
       " 'In what species did the COVID-19 virus likely originate?',\n",
       " 'What risk factors should be considered in addition to clinical symptoms?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve all the questions and answers\n",
    "qas = data[\"qas\"]\n",
    "\n",
    "# show the first question-answer pair. Note the answer starts at the 6117th character\n",
    "print(qas[0])\n",
    "\n",
    "# get all questions\n",
    "qs = [item[\"question\"] for item in qas]\n",
    "qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, following the instructions below step by step to develop the QA system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Tokenizer\n",
    "\n",
    "Define a function `tokenize(doc)`  as follows:\n",
    "   - Take a piece of text (i.e. variable `doc`) as an input\n",
    "   - Split the input text into unigrams\n",
    "   - Clean up tokens:\n",
    "       - Lemmatize all unigrams\n",
    "       - Remove all stop words\n",
    "       - Remove all punctuations\n",
    "       - Convert all unigrams to the lower case \n",
    "       - remove empty unigrams\n",
    "   - Return the list of unigrams after all the processing. (Use spacy or stanza package. Test stop word or punctuation, check https://spacy.io/api/token#attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "def tokenize(doc):\n",
    "    # add your code\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    stopwords = nlp.Defaults.stop_words\n",
    "    doc_tokens = nlp(doc)\n",
    "    sp_tokens = [i.lemma_ for i in doc_tokens]\n",
    "    tokens=[]\n",
    "    pun = string.punctuation + '—'\n",
    "    tokens = [i.strip(pun) for i in sp_tokens]\n",
    "    tokens = [i.strip() for i in tokens if i.strip() != '']\n",
    "    tokens = [i.strip() for i in tokens if i.strip() not in stopwords]\n",
    "\n",
    "\n",
    "    # add your code\n",
    "                       \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['old', 'people', 'people', 'age', 'severe', 'chronic', 'medical', 'condition', 'like', 'heart', 'disease', 'lung', 'disease', 'diabetes', 'example', 'high', 'risk', 'develop', 'covid-19', 'illness']\n"
     ]
    }
   ],
   "source": [
    "doc = 'Older people and people of all ages with severe chronic medical conditions — \\\n",
    "like heart disease, lung disease and diabetes, \\\n",
    "for example — seem to be at higher risk of developing serious COVID-19 illness.'\n",
    "\n",
    "print(tokenize(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Compute TF-IDF Matrix\n",
    "\n",
    "Define a function `compute_tfidf(docs)` as follows: \n",
    "\n",
    "- Take `docs`, a list of documents (e.g. a list of questions) as an input\n",
    "- Tokenize each document in `docs` using the `tokenize` function defined in 3.1. \n",
    "- Calculate tf_idf weights as shown in lecture notes (Reuse the last code segment in NLP Lecture Notes (II))\n",
    "- Return a smoothed normalized `tf_idf` array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "import pandas as pd\n",
    "def compute_tfidf(docs):\n",
    "    docs_token = [tokenize(doc) for doc in docs]\n",
    "    docs_tokens = {idx: nltk.FreqDist(token) for idx, token in enumerate(docs_token)}\n",
    "\n",
    "    # since we have a small corpus, we can use dataframe\n",
    "    # to get document-term matrix\n",
    "    # but don't use this when you have a large corpus\n",
    "\n",
    "    dtm=pd.DataFrame.from_dict(docs_tokens, \\\n",
    "                               orient=\"index\"\n",
    "                              )\n",
    "    dtm=dtm.fillna(0)\n",
    "    # sort by index (i.e. doc id)\n",
    "    dtm = dtm.sort_index(axis = 0)\n",
    "    # convert dtm to numpy arrays\n",
    "    tf=dtm.values\n",
    "    # sum the value of each row\n",
    "    doc_len=tf.sum(axis=1)\n",
    "    # divide dtm matrix by the doc length matrix\n",
    "    # note broadcasting\n",
    "    tf=np.divide(tf, doc_len[:,None])\n",
    "    # set float precision to print nicely\n",
    "    np.set_printoptions(precision=2)\n",
    "    # get document freqent\n",
    "    df=np.where(tf>0,1,0)\n",
    "    # get idf\n",
    "    idf=np.log(np.divide(len(docs), \\\n",
    "            np.sum(df, axis=0)))+1\n",
    "\n",
    "    smoothed_idf=np.log(np.divide(len(docs)+1, np.sum(df, axis=0)+1))+1\n",
    "    # step 6. get tf-idf\n",
    "    s=tf*idf\n",
    "    tf_idf=normalize(tf*idf)  \n",
    "    smoothed_tf_idf=normalize(tf*smoothed_idf)\n",
    "    # add your code\n",
    "    \n",
    "    return smoothed_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41, 0.41, 0.41, 0.41, 0.41, 0.41, 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.61, 0.8 , 0.  , 0.  , 0.  ,\n",
       "        0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.36, 0.  , 0.47, 0.47, 0.47,\n",
       "        0.47]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function using three questions\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "compute_tfidf(qs[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Put Everything Together (bonus question)\n",
    "\n",
    "Define a function `find_solutions(qs, article)` as follows: \n",
    "\n",
    "- Take two inputs:\n",
    "    - `qs`: a list of questions (i.e. strings)\n",
    "    - `article`: a document which may contain answers to the questions\n",
    "- Segment the article into sentences (i.e. `sents`).Locate the sentence which can answer a question.\n",
    "- Concatenate the questions (`qs`) and sentences (`sents`) into a single list (i.e. `qs + sents`)\n",
    "- Call the function `compute_tfidf` in 3.2 with `qs + sents` to get a `TF-IDF` matrix. (Note, now `qs` and `sents` are converted to TF-IDF vectors in the same dimension. As a result, measure their similarities.) \n",
    "- Split the `TF-IDF` matrix into two sub matrices, one corresponding to `qs` and the other for `sents`. \n",
    "- Next, calculate the pairwise cosine similarity between the `qs` and `sents`. With $m$ questions and $n$ sentences, get a $m \\times n$ matrix. ( `sklearn.metrics.pairwise_distances` to calculate pairwise distances between two matrices)\n",
    "- Finally, the answer to each question is the sentence which has the `maximum similarity` to it. \n",
    "- Print out each question and its matched answer. Check if QA system is able to find the right answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "import sklearn\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def find_solutions(qs, article):\n",
    "    dtm = compute_tfidf(qs + nltk.sent_tokenize(article))\n",
    "    qsmat = dtm[:len(qs)]\n",
    "    ansmat = dtm[-len(nltk.sent_tokenize(article)):]\n",
    "    similarity = 1 - pairwise_distances(qsmat, ansmat, metric='cosine')\n",
    "    sorted = np.argsort(similarity)[:, -1]\n",
    "    mylist = [['Question: ' + qs[i], 'Answer: ' + nltk.sent_tokenize(article)[sorted[i]]] for i in range(len(qs))]\n",
    "\n",
    "    return mylist\n",
    "    # add your code\n",
    "                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Question: What age group has the highest rate of severe outcomes?',\n",
       "  'Answer: A CDC Morbidity & Mortality Weekly Report that looked at severity of disease among COVID-19 cases in the United States by age group found that 80% of deaths were among adults 65 years and older with the highest percentage of severe outcomes occurring in people 85 years and older.'],\n",
       " ['Question: How is COVID-19 spread?',\n",
       "  'Answer: Does the patient reside in an area where there has been community spread of COVID-19?'],\n",
       " ['Question: How many states in the U.S. have reported cases of COVID-19?',\n",
       "  'Answer: All 50 states have reported cases of COVID-19 to CDC.'],\n",
       " ['Question: When did the White House launch the \"15 Days to Slow the Spread\" program?',\n",
       "  'Answer: CDC Recommends\\nEveryone can do their part to help us respond to this emerging public health threat:\\nOn March 16, the White House announced a program called “15 Days to Slow the Spread,”pdf iconexternal icon which is a nationwide effort to slow the spread of COVID-19 through the implementation of social distancing at all levels of society.'],\n",
       " ['Question: What should mildly-ill patients do?',\n",
       "  'Answer: People who are mildly ill with COVID-19 are able to isolate at home during their illness.'],\n",
       " ['Question: What type of virus is SARS-CoV-2?',\n",
       "  'Answer: The SARS-CoV-2 virus is a betacoronavirus, like MERS-CoV and SARS-CoV.'],\n",
       " ['Question: What viruses are similar to the COVID-19 coronavirus?',\n",
       "  'Answer: COVID-19 Emergence\\nCOVID-19 is caused by a coronavirus.'],\n",
       " ['Question: What are the phases of a pandemic?',\n",
       "  'Answer: The United States nationally is in the initiation phase of the pandemic.'],\n",
       " ['Question: At which phase does the peak of the pandemic occur?',\n",
       "  'Answer: The peak of illnesses occurs at the end of the acceleration phase, which is followed by a deceleration phase, during which there is a decrease in illnesses.'],\n",
       " ['Question: People with which medical conditions have a higher rate of severe illness?',\n",
       "  'Answer: Older people and people with severe chronic conditions should take special precautions because they are at higher risk of developing serious COVID-19 illness.'],\n",
       " ['Question: What kind of test can diagnose COVID-19?',\n",
       "  'Answer: CDC developed an rRT-PCR test to diagnose COVID-19.'],\n",
       " ['Question: In what species did the COVID-19 virus likely originate?',\n",
       "  'Answer: Coronaviruses are a large family of viruses that are common in people and many different species of animals, including camels, cattle, cats, and bats.'],\n",
       " ['Question: What risk factors should be considered in addition to clinical symptoms?',\n",
       "  'Answer: Factors to consider in addition to clinical symptoms may include:\\nDoes the patient have recent travel from an affected area?']]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the system\n",
    "\n",
    "find_solutions(qs, article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
